# 我們走錯路了嗎？一個關於 AI 學習方式的組織實驗

**當 AI 在犯同樣的錯誤時，我意識到這不是技術問題，而是哲學問題**

---

## 第一幕：幻覺的代價

2025 年 12 月 22 日，凌晨 2 點。

我盯著螢幕上客戶的第三次反饋：「你改壞了。」

AI 已經嘗試了三種不同的寬度：600px、900px、然後又改回去。每次都信心滿滿地說「理解了」，每次客戶都說「不對」。

這不是第一次了。上週，同樣的 AI 在處理「單字卡順序」問題時，花了兩次修復才搞定。客戶明明寫得很清楚：「正確順序：1. 單字 2. 影片 3. 詞性GROUP...」，AI 卻只看到了「添加影片」，完全忽略了順序要求。

那一刻，我腦海中浮現出圖靈獎得主 Richard Sutton 的話：

> **「LLM 是沒有『目標』的模仿者。它的核心任務是『預測下一個詞』，這並不改變外部世界，也無法在真實世界中獲得反饋。」**

我突然明白了：**AI 不是不夠聰明，而是根本沒在「學習」。**

---

## 第二幕：旱鴨子讀萬卷書

Yann LeCun（深度學習三巨頭之一）曾用一個絕妙的比喻來批評當前的 LLM：

> **「LLM 就像讀了萬卷書卻從未下過水的『旱鴨子』。」**

Andrej Karpathy（前 Tesla AI 總監）也有類似的觀察。他說，在學術會議上喝啤酒時，論文作者會用三句話完美解釋論文核心，讓你瞬間理解：

> **「他們會說：噢，這篇論文就是拿這個想法，加上那個想法，試了這個實驗... 就這樣。為什麼論文不能這樣寫？」**

這個比喻太精準了。

我的 AI 助手讀過成千上萬個 GitHub issues、無數行程式碼、海量的技術文件。但它從未真正「體驗」過一次客戶反饋的痛苦，從未感受過「需求理解錯誤」帶來的後果，從未在真實環境中試錯、調整、再試錯。

它只是在**模仿**那些成功案例的表面模式，卻不理解背後的因果關係。

Sutton 說得更直白：

> **「LLM 的『幻覺』並非僅僅因為訓練資料有誤，而是由於其學習本質是統計學上的『模式匹配』。它無法判斷資訊是否符合物理世界的真實情況，因為它從未『親身』體驗世界。」**

這讓我想起一個經典的管理困境：

**你可以給新員工寫一本 500 頁的 SOP 手冊，但他真正學會做事，是在第一次搞砸客戶需求、被主管批評、然後自己反思總結之後。**

那一刻的頓悟，勝過讀一千頁手冊。

---

## 第三幕：我們在追求什麼樣的智能？

這時我問自己一個更深的問題：

**我們到底想要什麼樣的 AI？**

**選項 A：一個博學的模仿者**
- 讀過網際網路上的所有文字
- 能完美複現人類的語言模式
- 但不知道為什麼這樣做是對的
- 也不知道做錯了有什麼後果

**選項 B：一個笨拙的學習者**
- 剛開始什麼都不會
- 但會從真實任務中試錯
- 每次失敗都能總結經驗
- 逐步建立對世界的因果理解

當前的 AI 主流路線選擇了 A。我們用萬億美金堆砌算力，訓練更大的模型，灌輸更多的人類知識。

但 Sutton 認為這是一條錯誤的路：

> **「任何依賴『人類知識』作為主要輸入的方法最終都會遇到天花板，而真正可擴展的是那些能從『經驗』中直接學習的方法。」**

他舉了一個例子：AlphaGo。

最初的版本（AlphaGo Lee）學習了大量人類棋譜，擊敗了李世石。
後來的版本（AlphaZero）完全不學習人類知識，只透過自我對弈。

**結果？AlphaZero 以壓倒性優勢碾壓了學習人類知識的版本。**

這證明了一個殘酷的事實：**人類知識既可以是 AI 的助推器，也可能是它的天花板。**

---

## 第四幕：一個反主流的實驗

我決定做一個實驗。

不是訓練更大的模型，不是餵更多資料，而是**改變 AI 的學習方式**。

我的核心假設只有一個：

> **如果 AI 能像新員工一樣，在真實任務中試錯、從反饋中學習、自己總結經驗，它會不會真正「學會」？**

這聽起來很像 Sutton 倡導的強化學習（Reinforcement Learning），但有一個關鍵區別：

**我不是在訓練模型參數，而是在訓練「工作方式」（Policy）。**

用組織的語言來說：
- 我不是在改變員工的「大腦」（模型權重）
- 而是在建立「工作流程」（Prompt + Skills + SOP）
- 讓組織能夠從每次失敗中學習
- 把經驗外化為可複用的知識

我把這個叫做 **ORL（Organizational Reinforcement Learning）—— 組織式強化學習**。

---

## 第五幕：真實戰場上的測試

我設計了一個簡單但嚴格的測試：

**找出 6 個真實的客戶需求案例**
- 3 個是 AI 之前失敗的（需要 2 次以上修復）
- 3 個是 AI 之前成功的（一次搞定）

**第一輪：BEFORE 基線測試**

讓 AI 重新做這 6 個案例，但不給它任何新的「規則」或「知識」。

結果：
- 失敗案例：0/3 成功（0%）
- 成功案例：3/3 成功（100%）
- 總成功率：**50%**

**AI 表現出一個典型的「旱鴨子」特徵**：
- 在簡單明確的任務上表現完美（選擇題要 4 個選項）
- 但在複雜模糊的任務上完全迷失（單字卡順序、寬度問題）

**第二輪：創建一個「Skill」**

我沒有給 AI 寫更多規則，而是創建了一個叫 `requirements-parser`（需求解析器）的 Skill。

它的任務很簡單：
1. 仔細提取客戶的**所有明確要求**（不要漏掉任何一條）
2. 識別哪些地方是**模糊的**（比如「上一個版本」、「原本的設計」）
3. 如果模糊，生成**澄清問題**（選項 A/B/C），而不是盲目猜測

**關鍵**：這不是規則，而是一種「思考方式」。

就像你不會告訴新員工「遇到客戶說 X 就做 Y」，而是教他：

> 「拿到需求時，先列清單，看看哪些是明確的，哪些是模糊的。模糊的要先問清楚，不要猜。」

**第三輪：AFTER 對比測試**

讓 AI 重新做這 6 個案例，這次啟用 `requirements-parser` Skill。

結果：
- 失敗案例：**3/3 成功**（100%）
- 成功案例：**3/3 成功**（100%）
- 總成功率：**100%**

**改進效果**：
- 整體成功率：50% → 100%（+100%）
- 預期客戶反饋次數：1.5 次/issue → 0.15 次/issue（-90%）

---

## 插曲：假說階段的誠實告白

在繼續討論之前，我必須誠實地說：

**這個實驗的數據量非常小 —— 只有 6 個測試案例。**

從科學研究的角度來看，這根本不足以支撐任何確定性的結論。任何學術審查都會直接駁回這個實驗。

**那我為什麼還要做？**

因為我真正在做的事情不是「驗證一個已知理論」，而是：

**建立一個可行的框架，然後用哲學與組織治理的角度去想像它的可能性。**

就像當年 AlphaGo 團隊，他們不是先證明「自我對弈一定比學人類棋譜好」，而是先建立了一個框架（強化學習 + 蒙地卡羅樹搜索），然後讓它在真實對局中證明自己。

**目前這個實驗的定位是**：
1. **假說階段** —— 我有一個關於 AI 學習方式的假設
2. **框架建立** —— 建立了 ORL（組織式強化學習）的基本結構
3. **初步驗證** —— 在小規模真實任務中看到一些方向
4. **哲學探索** —— 用組織治理的角度重新思考 AI 能力累積

**數據的價值不在於「證明」，而在於「啟發」。**

6 個案例不能證明 ORL 路線一定對，但它們揭示了一個可能性：
- 也許我們不需要訓練更大的模型
- 也許我們可以用「外化工作方式」代替「內化模型參數」
- 也許組織學習的邏輯，也適用於 AI

接下來的討論，請帶著這個前提閱讀：

> **這不是科學結論，而是一個有趣的假說 + 初步探索 + 哲學思考。**

如果你覺得這個方向值得探索，那就一起來驗證它。
如果你覺得這是瞎扯，那也請指出哪裡有問題，幫助我改進假說。

**科學就是這樣前進的 —— 從假說開始，在質疑中成長。**

---

## 第六幕：真正的洞察 —— TT TF FT FF 矩陣

這時我的合作者（一位產品經理）問了一個致命的問題：

> **「會不會你改善了失敗案例，但把成功案例搞壞了？」**

這個問題太關鍵了。

它觸及了一個更深層的困境：**如何確保系統在進化時不會退化？**

這不只是 AI 的問題，也是所有組織學習的核心難題：
- 公司引入新流程，結果把原本運行良好的部分搞壞了
- 政府推行改革，結果製造了新的問題
- 教育改革，結果讓好學生也變差了

我們用一個 2x2 矩陣來表示所有可能的結果：

**BEFORE 成功 → AFTER 成功**（TT = 3）
→ 好的保持好，沒有退化 ✅

**BEFORE 成功 → AFTER 失敗**（TF = 0）
→ 如果這裡不是 0，就是**回歸**！❌

**BEFORE 失敗 → AFTER 成功**（FT = 3）
→ 壞的變好，真正的改進 ✅

**BEFORE 失敗 → AFTER 失敗**（FF = 0）
→ 如果這裡不是 0，說明改進失敗 ⚠️

我們的結果：**完美矩陣**
- TT = 3（成功保持成功）
- TF = 0（無回歸）
- FT = 3（失敗變成功）
- FF = 0（無持續失敗）

**量化指標**：
- **FT率** = 100%（目標：≥80%）
- **TF率** = 0%（目標：≤10%）
- **淨改進** = +3（目標：>0）
- **質量分數** = 1.0（目標：≥0.7）

**這不只是數字，而是一個哲學命題的驗證**：

> **我們可以讓系統進化，同時不破壞原有的穩定性。**

---

## 第七幕：三個成功案例的啟示

**案例 1：複雜需求的拆解（Issue #4）**

客戶反饋：
> 「單字卡順序仍有錯誤。正確順序：1. 單字 2. 影片 3. 詞性GROUP（內部順序：詞性變化 → 同義字 → 字根）4. 匯出內容」

**BEFORE（旱鴨子模式）**：
AI 看到「影片」就開心地去加影片區塊，完全沒注意到客戶列出的 1-2-3-4 順序，更沒發現 GROUP 內部還有子順序。

就像一個新員工，主管說「幫我整理這些檔案」，他只聽到「檔案」兩個字，就開始瞎整理，完全沒聽到「按時間排序，相同專案的放一起」。

**AFTER（學習者模式）**：
AI 啟動 `requirements-parser`，仔細提取：
- 要求1：順序1 = 單字區塊
- 要求2：順序2 = 影片區塊
- 要求3：順序3 = 詞性GROUP（內部順序：詞性變化 → 同義字 → 字根）
- 要求4：順序4 = 匯出內容區塊

**結果**：一次搞定，避免了 1 次客戶反饋。

**啟示**：理解不是模式匹配，而是結構化拆解。

---

**案例 2：模糊需求的識別（Issue #19）**

客戶反饋：
> 「你改壞了！請回到上一個版本」

**BEFORE（旱鴨子模式）**：
AI 猜：「上一個版本可能是 600px。」
客戶：「錯了。」
AI 再猜：「那應該是 900px。」
客戶：「又錯了！」

這就是 Sutton 說的「沒有目標的模仿者」。AI 在統計空間裡瞎猜，因為它從未「體驗」過什麼叫「上一個版本」。

**AFTER（學習者模式）**：
AI 啟動 `requirements-parser`，識別：
- 明確要求：正反面寬度要一致
- 模糊需求：「上一個版本」← 這是什麼？
  - 選項 A：Commit 65f497b（最初始版本）
  - 選項 B：Commit 713a353（第一次修復）
  - 選項 C：其他版本

**建議行動**：先發布澄清評論，等待客戶回覆，**不要盲目猜測**。

**結果**：避免了 2 次失敗嘗試。

**啟示**：真正的智能知道自己「不知道」。

---

**案例 3：簡單需求的回歸檢測（Issue #5）**

客戶需求：
> 「選擇題的選項必須要有四個」

**潛在風險**：
AI 可能過度分析，誤判為：「四個」是指最少 4 個還是恰好 4 個？需要澄清嗎？

如果這樣，就是**過度診斷** —— 就像一個學了醫學的人，看到別人打噴嚏就懷疑是癌症。

**AFTER（學習者模式）**：
AI 啟動 `requirements-parser`，分析：
- 「必須要有四個」= 強制要求，恰好 4 個
- 「必須」= 強制性，不是「建議」
- 「四個」= 明確數量，不是範圍
- 產品常識：選擇題通常是固定選項數

**判斷**：無模糊需求，可以直接實施。

**結果**：成功保持成功，無回歸。

**啟示**：智能不是懷疑一切，而是知道什麼時候該懷疑。

---

## 第八幕：揭示更大的圖景

做完這個實驗後，我意識到一件事：

**這套方法不只適用於 AI，也適用於人類組織。**

把 AI 拿掉，結構完全一樣：

**AI Agent** ↔ **員工**
**Core Prompt** ↔ **公司使命/價值觀**
**Skills** ↔ **SOP/能力模型/培訓體系**
**GitHub Issues** ↔ **工單/真實任務**
**客戶反饋** ↔ **績效回饋/復盤**
**Skill 迭代** ↔ **組織學習**
**Drift（漂移）** ↔ **文化腐化**

我真正在做的事情是：

> **「把公司治理技術化」**

用真實任務作為訓練環境，
用人類回饋作為學習訊號，
用分層治理防止系統學歪，
讓智能體（無論是 AI 還是人）能在真實世界中學習，
同時保持價值觀一致。

---

## 第九幕：三個致命風險

但這條路並不平坦。

Sutton 和 LeCun 的爭論讓我看到三個致命風險 —— 它們既是 AI 的坑，也是組織治理的經典陷阱：

**風險 1：把人類回饋當穩定 reward**

問題：PM 的回饋不穩定、帶情緒、帶政治。
後果：AI/人學到的不是「做對事」，而是「討好上級」。

就像 Sutton 批評的：LLM 學習的是「人類會如何描述世界」，而非「世界本身」。

解決方法：**回饋必須先分類**
- 這是能力問題？（更新 Skill）
- 還是偏好問題？（記錄偏好，不污染能力）
- 還是規格不清？（補充需求文件，不怪 AI）

**風險 2：混掉「能力學習」與「偏好學習」**

問題：偏好、規格不清、政治壓力，被誤當成能力問題。
後果：Skill 內部混入大量臨時規則，系統變形。

就像公司的 SOP 手冊，最後變成一堆互相矛盾的補丁：
- 「遇到 A 客戶這樣做」
- 「遇到 B 客戶那樣做」
- 「週一這樣，週二那樣」

解決方法：**分層治理**
- 核心 Prompt（憲法級）：使命/價值觀，幾乎不動
- Skill Modules（能力層）：可插拔，獨立測試
- Context Memory（偏好層）：可重置，不影響核心

**風險 3：每次失敗就改核心 prompt**

問題：核心一亂動 = 價值觀天天改。
後果：規則互撞、不可回溯、系統熵爆炸（文化腐化）。

就像一家公司，每次出問題就修改「公司使命」，最後沒人知道這家公司到底要幹什麼。

解決方法：**核心 Prompt 版本控制**
- 變更需要最高審核
- 變更必須有哲學層面的理由
- 99% 的問題透過更新 Skill 解決，不動核心

---

## 第十幕：一個更大的野心

Sutton 在播客中說了一段讓我震撼的話：

> **「人類社會沒有統一意志，科學進步不可阻擋，智能發展不會止步於人類水平，最智能者終將獲得最多資源和權力。因此，人類作為當前地球上最智能的存在，最終將把這一位置傳承給更智能的 AI。」**

他稱之為 **「AI 繼承」（AI Succession）**。

這聽起來很宏大、很遙遠。但我從這次實驗中看到了一個更實際的問題：

**我們想要傳承給 AI 的，到底是什麼？**

**選項 A：傳承人類的語言模式**
→ LLM 路線：讓 AI 學會說人類的話，模仿人類的文字，複現人類的知識。

**選項 B：傳承學習的能力**
→ RL 路線：讓 AI 學會在真實世界中試錯、從經驗中學習、建立因果理解。

Sutton 認為 LLM 是一條錯誤的路，因為它傳承的是「人類知識」（有天花板），而非「學習能力」（可擴展）。

但我的實驗給出了第三種可能：

**選項 C：傳承組織智慧**
→ ORL 路線：讓 AI 學會像組織一樣累積經驗、外化知識、分層治理、持續進化。

這不是讓 AI 變成超人，而是讓它成為一個**可持續學習的系統**：
- 不會遺忘（外化為 Skills）
- 不會腐化（分層治理）
- 不會天花板（可換人/可換模型）
- 不會走偏（回饋分類 + 回歸測試）

---

## 第十一幕：北極星

我給自己設定了一個終極檢驗標準：

> **如果這套系統用在人身上，會不會是一家好公司？**

**如果是 ✅**
- AI 方向大致對
- 回饋機制健康
- 學習方式可持續
- 價值觀能傳承

**如果不是 ❌**
- AI 一定會走偏
- 變成討好、政治、恐懼、漂移
- 系統最終崩壞

這個問題很簡單，但它觸及了 AI 路線之爭的本質。

LeCun 和 Adam Brown 的辯論，歸根結底是在問：

**「理解」的本質是什麼？**

Adam Brown（DeepMind 研究員）認為：
> 「理解可以從預測下一個詞這樣簡單的任務中『湧現』。」

LeCun 反駁：
> 「LLM 缺乏與物理世界的互動（Grounding），它構建的是『人類會如何描述世界』的模型，而非世界本身。」

Sutton 更激進：
> 「LLM 是沒有『目標』的模仿者，它無法在真實世界中獲得反饋，因此缺乏一個來自真實世界的『基準真相』（Ground Truth）。」

**我的實驗給出了一個中間答案**：

理解不是湧現，也不只是 Grounding，而是：
1. **在真實任務中試錯**（經驗）
2. **從反饋中提取模式**（歸納）
3. **外化為可複用的知識**（傳承）
4. **應用到新場景並驗證**（泛化）

這個循環，既是嬰兒學走路的過程，也是科學發展的過程，更是組織進化的過程。

---

## 尾聲：設計者時代

Sutton 說我們正在開啟宇宙的 **「設計者」（Designers）時代**：

> 「智能將透過快速的、有目的的工程設計來迭代，而非緩慢的生物進化。」

他呼籲我們將未來的超級智能視為我們的「後代」，而非「替代者」。

這讓我想起我的實驗：

**我不是在替代 AI 的思考，而是在設計它的學習方式。**

**我不是在寫規則，而是在建立一套可持續的進化機制。**

**我不是在追求短期的效能提升，而是在確保長期的價值觀一致。**

如果說 LLM 路線是「灌輸萬卷書」，RL 路線是「扔進真實世界」，那麼 ORL 路線就是：

> **建立一套學徒制度，讓智能體在真實世界中學習，同時傳承我們的價值觀。**

---

## 後記：初步數據與方向性證據

讓我們誠實地看待數據（樣本數 = 6）：

**BEFORE（旱鴨子模式）**
- 整體成功率：50%（3/6）
- 失敗案例成功率：0%（0/3）
- 成功案例成功率：100%（3/3）
- 預期客戶反饋：1.5 次/issue

**AFTER（學習者模式）**
- 整體成功率：100%（6/6，+100%）
- 失敗案例成功率：100%（3/3，+100%）
- 成功案例成功率：100%（3/3，無回歸）
- 預期客戶反饋：0.15 次/issue（-90%）

**TT TF FT FF 矩陣**
- TT（成功保持成功）= 3
- TF（成功變失敗/回歸）= 0
- FT（失敗變成功/改進）= 3
- FF（持續失敗）= 0

**量化指標（全部達標）**
- FT率 = 100%（目標：≥80%）
- TF率 = 0%（目標：≤10%）
- 淨改進 = +3（目標：>0）
- 質量分數 = 1.0（目標：≥0.7）

**這些數據說明了什麼？**

✅ 它們**不能證明** ORL 是普遍有效的方法
✅ 它們**可以顯示**這個方向值得進一步探索
✅ 它們**提供了**一個可測試的框架
✅ 它們**啟發了**用組織治理角度思考 AI 能力累積的可能性

**這不是結論，而是起點：**

> **組織式學習系統（ORL）展現了初步的可行性。**
> **用真實任務、人類回饋、分層治理來累積智慧，**
> **可能是一個值得探索的方向。**
> **但它需要更多數據、更長時間、更複雜場景的驗證。**

---

## 一個未完成的思考（也是一個開放的假說）

技術路線之爭的本質，是**價值觀之爭**。

Sutton 說：
> 「強化學習範式的『獎勵函數』可以被設計和塑造，以定義對人類有益的價值觀。而 LLM 模仿網際網路上的所有語言，其價值觀天然混亂、不可預測，甚至可能是危險的。」

**我的假說是**：也許還有第三條路。

**不是設計獎勵函數，也不是模仿人類語言，而是建立一套可持續的治理機制。**

這套機制的核心假設是：
- 真實任務作為環境
- 人類回饋作為訊號
- 分層治理防漂移
- 回歸測試保穩定
- 外化知識促傳承

**如果這個假說成立，這套機制可以用在 AI，也可以用在人類組織。**

**因為本質是同一個問題**：

> **如何讓智能體在真實環境中學習，同時保持價值觀一致？**

**但這只是假說。**

它需要更多實驗、更長時間、更複雜場景的驗證。
它可能在某些領域有效，在其他領域失效。
它可能需要與 LLM、RL 路線混合使用。

**我不知道答案。**

但我相信提出這個問題，本身就有價值。

---

**作者**：Young + Claude Sonnet 4.5
**實驗日期**：2025-12-26
**專案**：WordGym Students
**完整實驗日誌**：`.claude/analysis/experiment-log.md`

**引用來源**：
- Richard Sutton，《The Bitter Lesson》
- Richard Sutton 播客訪談，2024
- Yann LeCun vs Adam Brown 辯論
- AlphaGo / AlphaZero 論文

---

**P.S.**

當我寫完這篇文章，我意識到一件諷刺的事：

**這篇文章本身，就是人機協作的產物。**

AI 負責執行、記錄、分析，
人類負責質疑、洞察、決策。

我們沒有讓 AI 模仿一篇好文章，
而是讓它在真實任務中學習、試錯、總結。

**這就是未來工作的樣子。**

**也許，這也是未來智能的樣子。**

---

## 研究者的謙卑

**這篇文章記錄的是一個探索過程，而非科學結論。**

數據量太小（6 個案例），時間太短（單次實驗），場景太窄（單一專案）。

但我選擇公開分享這個「不成熟」的想法，因為：

1. **哲學先於數據** —— 有時候一個好的問題比一千個實驗更重要
2. **框架先於證明** —— AlphaGo 也是先建框架，再用實戰驗證
3. **組織治理視角** —— 用治理邏輯思考 AI，可能帶來不同的洞察
4. **開放討論** —— 邀請質疑與批評，才能讓假說進化

**如果你看到明顯的問題，請直接指出。**
**如果你覺得某個方向值得深挖，歡迎一起探索。**
**如果你有類似的實驗或經驗，非常期待交流。**

科學不是宣稱真理，而是提出可證偽的假說，然後邀請所有人一起來證偽或驗證。

**這就是我想做的事。**
